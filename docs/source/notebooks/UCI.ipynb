{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Goals of this experiment:\n",
    "## - SGHMCHD is more robust with respect to different initial (smaller) stepsizes than SGHMC [x]\n",
    "## - SGHMCHD may outperform SGHMC with best stepsize over coarse grid in certain cases, even when initialized\n",
    "## badly (and will perform comparably most of the time.) [x]\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import sys\n",
    "from os.path import dirname, join as path_join, isdir\n",
    "from glob import glob\n",
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "sys.path.insert(0, path_join(\"..\", \"..\", \"..\"))\n",
    "from pysgmcmc_experiments.utils import (\n",
    "    rename, format_tex as format_stepsize,\n",
    "    format_performance,\n",
    "    format_sampler\n",
    ")\n",
    "\n",
    "RESULT_DIR = path_join(\"..\", \"..\", \"..\", \"cluster_results\", \"uci\")\n",
    "\n",
    "\n",
    "def load_results(directory):\n",
    "    directories = [\n",
    "        directory for directory in glob(\"{}/*\".format(directory))\n",
    "        if isdir(directory)\n",
    "    ]\n",
    "    \n",
    "    def load_json(filename):\n",
    "        with open(filename) as f:\n",
    "            return json.load(f)\n",
    "    \n",
    "    configurations = tuple(\n",
    "        load_json(path_join(directory, \"config.json\"))\n",
    "        for directory in directories\n",
    "        if \"_sources\" not in directory\n",
    "    )\n",
    "    \n",
    "    results = tuple(\n",
    "        load_json(path_join(directory, \"run.json\"))\n",
    "        for directory in directories\n",
    "        if \"_sources\" not in directory\n",
    "    )\n",
    "    \n",
    "    \n",
    "    # runs = defaultdict(lambda: defaultdict(list))\n",
    "    runs = defaultdict(lambda: defaultdict(lambda: defaultdict(list)))\n",
    "    for configuration, result in zip(configurations, results):\n",
    "        if result[\"status\"] == \"COMPLETED\":\n",
    "            sampler = configuration[\"sampler\"]\n",
    "            stepsize = configuration[\"stepsize\"]\n",
    "            \n",
    "            for dataset, dataset_result in result[\"result\"].items():\n",
    "                dataset_result[\"dataset\"] = dataset\n",
    "                runs[dataset][stepsize][sampler].append(dataset_result)\n",
    "            \n",
    "    return runs\n",
    "\n",
    "runs = load_results(RESULT_DIR)\n",
    "\n",
    "def results_for(metric_function, is_loss=True):\n",
    "    performances = defaultdict(lambda: defaultdict(dict))\n",
    "    for dataset, dataset_results in runs.items():\n",
    "        for stepsize, stepsize_results in dataset_results.items():\n",
    "            for sampler, sampler_runs in stepsize_results.items():\n",
    "                performances_ = np.asarray([\n",
    "                    metric_function(y_true=run[\"y_test\"],\n",
    "                                    prediction_mean=run[\"prediction_mean\"],\n",
    "                                    prediction_variance=run[\"prediction_variance\"])\n",
    "                    for run in sampler_runs\n",
    "                ])\n",
    "                mean_performance = performances_.mean()\n",
    "                std_performance = performances_.std()\n",
    "                performances[dataset][stepsize][sampler] = (mean_performance, std_performance)\n",
    "    \n",
    "    records = defaultdict(dict)\n",
    "\n",
    "    if metric_function.__name__ == \"log_likelihood\":\n",
    "        records[\"BostonHousing\"] = {\n",
    "            \"VI\": \"$-2.903 \\\\pm 0.071$\",\n",
    "            \"PBP\": \"$-2.574 \\\\pm 0.089$\",\n",
    "        }\n",
    "        records[\"YachtHydrodynamics\"] = {\n",
    "            \"VI\": \"$-3.439 \\\\pm 0.163$\",\n",
    "            \"PBP\": \"$-1.634 \\\\pm 0.016$\"\n",
    "        }\n",
    "        records[\"Concrete\"] = {\n",
    "            \"VI\": \"$-3.391 \\\\pm 0.017$\",\n",
    "            \"PBP\": \"$-3.161 \\\\pm 0.019$\"\n",
    "        }\n",
    "        records[\"Wine Quality Red\"] = {\n",
    "            \"VI\": \"$-0.980 \\\\pm 0.013$\",\n",
    "            \"PBP\": \"$-0.968 \\\\pm 0.014$\"\n",
    "        }\n",
    "    elif metric_function.__name__ == \"rmse\":\n",
    "        records[\"Boston Housing\"] = {\n",
    "            \"VI\": \"$4.320 \\\\pm 0.2914$\",\n",
    "            \"PBP\": \"$3.014 \\\\pm 0.1800$\",\n",
    "        }\n",
    "        records[\"Yacht Hydrodynamics\"] = {\n",
    "            \"VI\": \"$6.887 \\\\pm 0.6749$\",\n",
    "            \"PBP\": \"$1.015 \\\\pm 0.0542$\"\n",
    "        }\n",
    "        records[\"Concrete\"] = {\n",
    "            \"VI\": \"$7.128 \\\\pm 0.1230$\",\n",
    "            \"PBP\": \"$5.667 \\\\pm 0.0933$\"\n",
    "        }\n",
    "        \n",
    "        records[\"Wine Quality Red\"] = {\n",
    "            \"VI\": \"$0.646 \\\\pm 0.0081$\",\n",
    "            \"PBP\": \"$0.635 \\\\pm 0.0079$\"\n",
    "        }    \n",
    "    else:\n",
    "        raise NotImplementedError(\n",
    "            \"We do not have any literature results for metric function {}\".format(\n",
    "            metric_function.__name__\n",
    "        ))\n",
    "    methods = set((\"VI\", \"PBP\"))\n",
    "\n",
    "    for dataset, dataset_results in runs.items():\n",
    "        for stepsize, stepsize_results in dataset_results.items():\n",
    "            for sampler, sampler_runs in stepsize_results.items():\n",
    "                print(\"DATASET:\", dataset)\n",
    "                relevant_performances = [\n",
    "                    mean_performance for mean_performance, _ in\n",
    "                    performances[dataset][stepsize].values()\n",
    "                ]\n",
    "                optimum = min if is_loss else max\n",
    "                method = format_sampler(sampler, stepsize)\n",
    "                mean_performance, std_performance = performances[dataset][stepsize][sampler]\n",
    "                records[rename(dataset)][method] = format_performance(\n",
    "                    mean=mean_performance,\n",
    "                    stddev=std_performance,\n",
    "                    bold=mean_performance == optimum(relevant_performances) # winner gets boldfaced\n",
    "                )\n",
    "                \n",
    "                methods.add(method)\n",
    "                \n",
    "    def method_key(method):\n",
    "        if method == \"PBP\":\n",
    "            return -float(\"inf\")\n",
    "        elif method == \"VI\":\n",
    "            return -1000000\n",
    "        else:\n",
    "            import re\n",
    "            stepsize_exponent = float(re.search(\"\\{\\{(-[0-9]+)\\}\\}\", method).group(1))\n",
    "            return (0.00001 * method.startswith(\"SGHMCHD\")) + stepsize_exponent\n",
    "    \n",
    "    columns = [rename(dataset) for dataset in DATASETS]\n",
    "    dataframe = pd.DataFrame.from_records(\n",
    "        records, columns=[rename(dataset) for dataset in DATASETS],\n",
    "        index=sorted(methods, key=method_key) \n",
    "    )\n",
    "    dataframe.metric_name = rename(metric_function.__name__)\n",
    "    dataframe.short_metric_name = metric_function.__name__\n",
    "    return dataframe\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute dataframes with results for all metrics and write them to files as latex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'results_for' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-92f4306c52c6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mmetric_function\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmetric_functions\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m     \u001b[0mmetric_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresults_for\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetric_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_loss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetric_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m==\u001b[0m \u001b[0;34m\"rmse\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetric_results\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0mresults_tablecode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprettyprint_results\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetric_results\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'results_for' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "import string\n",
    "\n",
    "# XXX: Insert line between \"literature results\" and our results\n",
    "def prettyprint_results(dataframe):\n",
    "    table = dataframe.to_latex(escape=False).replace(\"{}\", \"Method/Dataset\")\n",
    "   \n",
    "    table = table.rstrip(\"\\n\")\n",
    "    caption = \"\"\"\\\\caption{{{metric} for regression benchmarks from the UCI repository. \n",
    "For comparison we include results for VI~(variational inference) and\n",
    "PBP~(probabilistic backpropagation) for Bayesian neural network\n",
    "training taken from Hern√°ndez-Lobato and Adams~\\\\cite{{uci-vi-pbp}}.\n",
    "We report mean $\\\\pm$ stddev across $10$ runs.}}\"\"\".format(metric=dataframe.metric_name)\n",
    "    return \"\"\"\n",
    "\\\\begin{{table}}[H]\n",
    "{table}\n",
    "{caption}\n",
    "{label}\n",
    "\\\\end{{table}}\n",
    "\"\"\".format(table=table, caption=caption, label=\"\\label{tab:\" + dataframe.short_metric_name + \"}\")\n",
    "\n",
    "def rmse(y_true, prediction_mean, **kwargs):\n",
    "    return np.sqrt(\n",
    "        mean_squared_error(y_true=y_true, y_pred=prediction_mean)\n",
    "    )\n",
    "\n",
    "\n",
    "def log_variance_prior(log_variance, mean=1e-6, variance=0.01):\n",
    "    return np.mean(np.sum(\n",
    "        -np.square(log_variance - np.log(mean)) /\n",
    "        (2 * variance) - 0.5 * np.log(variance), axis=1\n",
    "    ))\n",
    "\n",
    "\n",
    "\n",
    "def log_likelihood(y_true, \n",
    "                   prediction_mean, prediction_variance,\n",
    "                   epsilon=1e-7):\n",
    "    n_datapoints, _ = batch_size, _ = np.shape(y_true)\n",
    "    f_mean = np.reshape(prediction_mean, (-1, 1))\n",
    "    f_log_var = np.reshape(prediction_variance, (-1, 1))\n",
    "    f_var_inv = 1. / (np.exp(f_log_var) + epsilon)\n",
    "    \n",
    "    mean_squared_error = np.square(y_true - f_mean)\n",
    "    \n",
    "    log_likelihood = np.sum(\n",
    "        np.sum(\n",
    "            -mean_squared_error * (0.5 * f_var_inv) - \n",
    "            0.5 * f_log_var, \n",
    "            axis=1\n",
    "        )\n",
    "    )\n",
    "    log_likelihood /= batch_size\n",
    "    log_likelihood += log_variance_prior(f_log_var) / n_datapoints\n",
    "    return log_likelihood\n",
    "\n",
    "y_true = np.random.rand(10, 1)\n",
    "y_pred = np.random.rand(10, 2)\n",
    "log_likelihood(y_true, y_pred[:, 0], y_pred[:, 1])\n",
    "\n",
    "from os.path import join as path_join\n",
    "\n",
    "OUTPUT_DIRECTORY = \"/home/moritz/thesis_repos/masterthesis_report/sghmchd/experiments/results/tables/uci\"\n",
    "# metrics all take \"y_test, prediction_mean, prediction_variance\n",
    "# as argument and return a scalar loss value.\n",
    "metric_functions = (rmse,)\n",
    "\n",
    "for metric_function in metric_functions:\n",
    "    metric_results = results_for(metric_function, is_loss=metric_function.__name__== \"rmse\")\n",
    "    print(metric_results)\n",
    "    results_tablecode = prettyprint_results(metric_results)\n",
    "    print(results_tablecode)\n",
    "    output_filename = path_join(\n",
    "            OUTPUT_DIRECTORY, \"{metric}.tex\".format(metric=metric_function.__name__)\n",
    "        )\n",
    "    with open(output_filename, \"w\") as f:\n",
    "        \n",
    "        f.write(results_tablecode)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
